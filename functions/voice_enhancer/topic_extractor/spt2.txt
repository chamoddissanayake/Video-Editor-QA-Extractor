Hello. Good afternoon. Okay. Can you hear me talk to. Thanks. Okay. A few more to join, but I think Let's start. They will join it. I think we have done a few a few analytics related topics in previous lectures. So we need Association related things, then classification some algorithms and then cluster in one of the algorithm. So basic things related to analytics we have done. But when we look at life cycle of analytics related projects, so we want to do the data preprocessing. So therefore, I thought of talking about some technique related to data preprocessing in this lecture. Let's see some of the techniques we use data preprocessing. So before we use use our data with the modules. So we want to do some for the processing and prepare our data sets to fit into data models. So some of the things we can discuss with this session. All right. So this is some overview to data preprocessing. Mainly we are talking about data quality and the main task we doing data Privacy. So under the texture, I will mainly focus on data cleaning and data. Yeah, mainly data cleaning. So in addition to that, data integration, data reduction, data transformation and discretization. So that's also we want to discuss. So mainly we will discuss data cleaning and data transformation and data discretization. So other two things, data reduction and integration, also in the slide set. But the main focus is other two topics. All right. One question. Do we need to upload the assignment to the. So I will discuss it later. So it's not an assignment to complete the assignment. You have time, but I'm not asking you to complete it, but I need to the group details and so on for that. Actually, that will has given not to submitted assignment. Answer. Okay. I will have to do to spend some time on the lecture so we can move into the assignment related things and we can discuss. And after that, I will give you some time to work on assignment today because since not on campus classrooms and then you don't meet physically. So then I want to do some time from the lecture time. So then you can get together, discuss about assignment after complete the lecture. So I will sign time for that. Okay. So first before discuss the main topic, if I explain you very briefly, this KDD process or knowledge discovery database, this process. So then you can see data preprocessing in to where it. So we collect data from different data sources. And then after that, we are creating a data set for our targeted problem. And after that, that data set, we want to further process to us in our models. So then data preprocess incomes there, then under data preprocessing, actually transformation also there. And this transform data will be used for machine learning data mining models. And after that, we want to derive patterns or new knowledge from that to process data. So data preprocessing, please. Some up here. Alright. So the main, the main main point we are trying to address with proposes needs. So we are trying to increase the quality of data that is important to get accurate results from our models. Right? So when we analyze we should expect accurate results. So the accuracy of the results highly depends on the quality of the data. So then the quality of the data , how to increase that's the problem we are trying to interest as in the data preprocessing pre processing is to improve the quality of our data. So then how to check the quality of our data. There are different ways to do that. The measures for quality data quality. One is accuracy, accuracy or the current or on our data set. So accuracy of data is one measurement to check the quality of the data. Then the other thing is that completeness. So we have the entire data set. So it's a partial data set. That's another measure. So we are talking about data quality, how to check the data quality to check the data quality. Accuracy is one measurement and the completeness is another measurement. Right. And the consistency. So the data some modified, some may not modify maybe same information in different places in a different way. So maybe Theta is not consistent, then quality goes down. So that's another measurement whether our data is consistent or not. And the time leaders. So where the data is up to date before analyze, if you analyzing very old date and talking about current situation. So then there's a problem with that result. So then the time leaders is another measurement for the quality and the believability. So how much we can trust our data that all over another measurement to check the quality of data. Because sometimes maybe we get data we don't know whether it is we can trust or not. We can trust the data set or not, like maybe before the academic exercises. So sometimes you download data from the web and you are trying to come to some conclusions, but we don't know those data is actual data or correct data or not. We should be able to trust data then only we can come to a good outcome. And the other thing is interpretability. So that's another important aspect. Developers should be able to interpret the data set. Sometimes maybe you just take the data and feeding into models and you get some output, but you can go back and explain what it is. Right? So then the data should be some interpretable format. Otherwise problem, what I'm explaining is quality quality of data we can measure using different measurements of accuracy, completeness, consistency, timeliness, reliability of interpretability. Please help you measure us to check the data quality. So then while we are pre processing our data. So what we are trying to do is bye increase. We are checking these accuracy measures or quality measures. Right. So using this quality images, we are checking our data is a point. Data set is not so that's the target of pre processing. Major things are major technical things we do to pre process data. One is data training and another point is data transformation and data discretization and data reduction and data integration. So these are the main four cars comes under data preprocessing, cleaning, transformation and discretization, data reduction and data integration. Right. So if I explain first briefly. So data cleaning interest the problems of missing values, the noisy data, identify your remove outliers. So remove inconsistencies. So those things comes under data cleaning and data transformation and data discretization. Under that we do mainly normalization built in concepts. Hierarchies. So that kind of things come under transformation in the same your data, you are transforming into different escapes your GPA from zero to 4 for analytic purpose. So I want to scale GP to zero to 10. Right. So we can transform our data set into different scale. And then another thing is data discretization. So when you get numeric data so you can break into ranges. So we can take some examples and discuss later. So here I want to tell you the main the manager has be doing data preprocessing, data cleaning, data transformation and data discretization. Then the data reduction. So data reduction. It can be dimensional reduction, numeral 50 production or data compression. So using these techniques, we are trying to reduce the dimensions or number of values, the size before processing data data integration mainly. So you get data from different sources. We want to combine them and prepare data set for processing. So then you able to see here these main pre processing tasks comes under interesting different anger, like cleaning. The one side transformation is another direction and the production is another angle. Integration is another angle. This mean pre processing techniques. Right. Next this lecture I will mainly talk about data training, transformation and discretization. Then some techniques comes under data reduction and integration. You can read a little bit. So those are also important. But I will not spend much time on those things. So Let's see some of the techniques available for a training. So if you look at data cleaning. So some of the things are related to the data in real world is dirty. So when you take data into whatever project. So most of the time it is 33 and there may be a lot of reasons to become Pat is dirty. So as an example, a lot of potential incorrect data. Maybe we are using some devices device, maybe have some faulty and generating incorrect data. And then the maybe some human error has done or some transmission due to various reasons. So we make it incorrect data. So give me a second. So problem with my piece, I can't get the pin. Okay. So then incomplete data. If you look at that is because lack of attribute value or like in certain attribute of interest, or if you take an example. So maybe in the dataset occupation just missing values. So the incomplete data, then the noisy data. Maybe like sometimes maybe when you do data entry. So maybe for some fields. So you just enter some sample data without thinking my salary that cannot be right. So then generating noise in data. So the noise contains noise or errors or outliers belongs to one data. Then the inconsistent data says as example, but this 2,010 some date and age is 42 incorrect inconsistent data. So maybe some ratings in one place 1-2-3 another place, same rating ABC inconsistent. So like that kind of clean. Maybe we want to do some intentional that mistakes may be there. So maybe someone don't like to give the birthday and January first is birthday. It's intentionally done data, but it's wrong. So then we want to clean them. So cleaning. We want to cleaning data due to these various reasons. That's what trying to explain this slide. So again talking about how this incomplete or missing data happening, I think if you just read this you can understand. So how to handle this machine data. So a few things we do before we start our project, one is ignore the topper. So when we have machine values in a topper, so simply you can ignore that one. That's one thing we do but we want to know some of the information important information. Maybe we are losing when you follow that app. So therefore you want to be careful. But when you elect ignore topples, maybe you are ignoring the topper because of one missing value. Maybe you are losing some other important data in there . The second point is filling the machine value using manually. So maybe you are going through the data set. So manually entering suitable value. That's a good option but may be very difficult. It as when you have huge data set. Another thing we do, we can do certain things to automatically fill up those machine values. Say Let's say class in a data set, a classification problem class field is some values machine. So then you can create a new class and maybe you label as unknown. Unknown. So the prediction comes unknown that we know some problem with the data. Or you can get the mean value for that attribute and mean value you can use to fill up the machine value. Alright, attribute mean. So maybe for category base you can take me in values and fill up. Another method is you can use some algorithm. So predictive algorithms. So decision tree ovation formula kind of predictive method to fill up the missing values. Those are the sound techniques we use to fill the missing values to try out this thing practically. So I have prepared a practical set in order to be logged into course. So I want you to go through this. Let see and apply these techniques and see how it is working. Me. Let me show you a technical. Can you see the practical? It is still the latest slide. I have two flights I think now you can see. Right. So this practical sets actually based on the car a tool. So I think I explained you introduce you wear sometimes back so you can use Vaca and apply different filters and try out with this practical set. So how to fill up machine values you can try out. So it's a step by step guide. I think I don't want to spend time during the lecture time so you can necessarily do it step by step guide. So like a few practical seat I have combined with this lecture. So I will give you some idea about pre processing app. After that I will give you time to try out these things. And so these things actually make base practical. It. I will give you some practicals for Python also. So then maybe that's useful to someone. So then because it's different when you use Python, you want to do a little bit of coding . So if you're not good in coding, so you can use we can see how it is working. If you are good in coding. So you can do the Python examples and see how it is looking. So that practical set for actually how to fill up missing data. And after complete that one, I will give you another practical is to work with noisy data. So then for that mainly to remove noisy data, a few techniques available. Ben and I will explain you in detail with another section in the same vector. Meaning what we do. Right. We take our date set and break into a few beans, and then we can if I tell you, we help you techniques, those few techniques we can use into few data mining tasks like. So I have mentioned here been in recreation clustering things like that. So then this same technique being meeting, we can use for data transformation as well. Like that. So then now here we are. We can talk about to remove on data. Right. So what they do mainly they break into beans, and after that we can get some smooth by mean value, but we take the mean value for one particular being. So then that outliers. We can automatically all the noise data up to a certain level goes out and we can get that in value. So then smooth by median. So you can get the median or you can get boundaries. Call me those things actually we can use to remove noisy data. So binings one of the techniques we can use to remove is data. And then the regression we can use regression means some regression cure is there. Then if we see some outliers data like this, so then if some noise out lies is there. So then we can see it is deviated from the our curve, our regression function. So then we can identify that. So we can remove it and we can do some treatment to solve the problem. So clustering also another way to remove easy data. After we form multiple clusters, we can easily identify. Okay. Alright. Yeah. So mainly with noisy data or when we preprocess data to remove noise, we should be able to catch outlier. So I have given here tactical seat. So then that mainly shows you how to remove outliers. So that's a practical. Then you can spend some time on this practice to get the idea how to remove outliers. So when you remove out lies so they automatically actually noise data goes South. Then the next point I go to discuss little live data transformation. And after that I will move into the data discretization. So beta transformation. Mainly we will use some function that function. It will transform data into different escape, a function that maps the entire set of values of a given attribute to new set of replacement value such that all value can be identified with one of the new value. Alright. See ethic of say you have some data set attribute. Let's see what we have from one to 10 . Say range is one to 10. So you want to transform that into thousand 2 10,000 range. So the one we want to represent here in this range. So we want to represent in this range like we are transforming our data into different. So the material available for that. So mainly what we do, we do normalization. So we use normalization forward to transform data into different scale. Normalization scale for within a small specific trains. So under normalization. So there are three normalization techniques. Min Max normalization is a discount normalization and normalization bite decimal. Scary. So these are the three techniques we use to a normalized. So normalizing data for to transform into different scale. All right. So in addition to that, when we do is moving and other things. Also actual data will transform into different scales. So here mainly I want to discuss the normalization actually. So if we look at this min Max normalization, they use this particular formula. So then in this notation, we have new minimum value, new maximum value. If that means in the new range , what is the minimum and maximum? We want to know in addition to that, we want to have the where you said we want to transform. Right. So we can use this form you to compute that. You can see one example here. Let the income range. So we want to set income range into 12,002 98,000. That normalize to zero to 1 trade. So our income is 12,000 to 90,000 range. So we want to bring down that into zero to 1. Right. So then here you can see 73,000 converting into this zero to 1 train. So this is the value. So then it's just a matter of applying values into this formula. So then the minimum value and the maximum value. Right. So then he'll be help me new value, mean all value. And so if you just have flight, so they need to be there converting to a train. So it's mean back and normalization. I think if you read this one, you can easily understand any questions. Do you want me to explain in detail these things or you can understand it. Right. All right. Okay. So then this is normalization what they do. They are using a formula like this to transform into different escape. So here we use the mean and various of the data set so that you should not mean and the variance mean and the variance. So any given value, you can convert into new scale. Right. So that we call it a disco normalization. The other one is normalization by basically scaling. So they use this formula, but basically they do. After any number we are trying to represent a email value. Then after you get some attribute with the list of values, so you want to find out is the smallest integer such that the maximum value that becomes less than zero. That means the decimal value. As example, say, if you want to convert this 73,000 decimal value, we want to divide that from 100,000. So then becomes five. Right. It's like that 80,000 to convert into the Gmail value. So you won't divide by 100,000 in becomes five. So then for the given data set. So you want to find out what is and divide present and convert into a so normalization. Bay, basically scaling another technique. So these are the three techniques comes under normalization. So that is to transform data into different scale. So then how to do this in practically. Again, I have shared a practical see with device guide and some sample data is also there. So you can go through that one. Then another important point is data discretization. So mainly we do this with continuous data. So we have three type of attributes. So maybe you heard about these things. Maybe under grade level, a three type of attribute, nominal attribute ordinance and numeric category. Maybe. I also mentioned this with classification. Nominal means, we can identify data uniquely uniqueness is there? But there is no order or your content. Or you can't do so it's just distinct values. As example, male, female. It's just, um, so you don't see any order there . You can't add subtract or you can't do anything with just unique a nominal value. So the ordinal that has uniqueness plus order. So then if we take search size. So the small, medium large, so unique values and also arise the subject grade ABC unique values. We have an order also. So its ordinary data. So other thing is numeric data. Right. So you get integers and real numbers. So these are the data we have. So description divide the range of continuous attribute into interval. So when we get continuous data. So then we use mainly discretization. So that is to create cover data in two ranges. Let's say as example of someone's say rainfall, rainfall, millimeters. So maybe Let's say that it's from zero to 200. Something like that say we want to break into ranges like low, medium and high , something like that. So then we can define ranges. So zero to 2. Now 2 2 75 is medium and 75. And above we say highlight. So you can set a quality data discretization. There are some ways of doing this. Right. So then there are some methods for this to data discretization, some method available. So again, one of the popular method for data discretization is being and histogram analysis is another technique. And cluster analysis. Decision tree analysis and correlation is like a lot of techniques are there. So I will show you how to do this with meaning. Right. So if you take been in I told you previously, also, we want our given data set. You want to break into a few beans and we smooth the data of the beans and so on. So what? We are like two type of being techniques there. Mainly we call it equal it or else it's equal distance. The other one is we call equal dip. Or we call equal frequency. Two types of bin techniques. One is equal with the one is equal depth. Right. So they both have actually some plus point minus point. And so on. So equal with what they do, basically divide the range into an intervals of equal size. Let's as example, Let's say we have 100 values, say we need five beans. We have five beans. So then we can say, okay, 0-2-1. So then 20 to 40, another win. So then 42 16. Right. You can get equal five in. Just to avoid this overlapping, we can say save up to 90. So then this 20 39. So then the 42 59 less than 62 89 we can break into. So then it's like equal week equal this day. So then one of the problem here is if data set is keyed, then what happens? So most of the data points belongs to one of the right. So that's one of the problem with this equal with billing method of data. It is, Let's say most of the zero to 100 values, and it's say most of the people in range 32, 40. So then a lot of entries comes into one. It's not then equally distributed. So such problems will be there. So then when we average and when we take a representative value for that particular being, you know, a lot of values we lost, so many people will represent just one variant. Then it will be a problem for the accuracy. The equal death is another way. So then they will be mainly considered the frequency. The main point is then in each bin maybe help five mins. Each bin say four bin. So each bin we want the same number of same per say this one has 5 4 data points and other one. Also, 5 data points been a same number of data points, same frequency. So that's equal date of equal frequency . Meaning. So device the range into an integer. Each contains approximately same number of samples. Good for data scaling. Manage in categorical attribute can be tricky. Right? So the same idea trying to explain here with one example, say we have a data set with zero for some data here. Right? So if you go for equal link, so then zero to 28 values are there. So then we can come up with range less than 10 values, then 10 to 20 and about 20. So we can define ranges and can assign value. So then when I assign our data into these ranges, so then the bill one has 2 values in two has 4 values and B three has 3 data points. Data points are not equally distributed, but equal frequency or equal being technique. If you take their main concern is always each bin we want to have same number of data points frequent, we must say so accordingly, we can find out what are the ranges for equal equal frequency binning. Alright. So if you take less than 14, 14 to 21 and 21, and above you can see each it has the same number of data points. So then if we represent that in a graph, a chart, so you can see equal by 10 equals frequency behavior. So winning method, actually, we can use for data on not just for discrete discretization. Right? So then after we break into beans, it can be equal dip or equal frequency or equal with then anything you can smooth the smoothing techniques. Actually, what we can do, we can take I mean value, say as example, we can see here three beans. So then if you take the mean value of the bean one, so it will be nine. So then to represent in one you can use nine. So then all the data points you can convert to nine. So then that way you can reduce the dimensionality or number of data points. So you need only just one. Really. And like second Ben, you can see 23 get her mean value. So then we can have a representative value that is a mean. Or you can take the media also. And another method is you can take the bean boundaries. So you can you can take the data points of that particular bin. Like if you take the first bin you can see we say, okay, four and 15. So this close to four and 9, also close to four and then 15. So we have four and 15. So if you take 21 and 25 as boundaries 24 plus to 25, and then that becomes 25 and another one is 21. So like we can take boundaries of the bean and close the value set into the boundary. So that's another way of actually it's more in data. So then it's discretization data plus is Modi. So same thing. You can do it when we do the noise removal. So maybe you can by mention remove noise. One technique binning. So then you can say beans like this. And after that you can use is moding techniques and then actually no information. Also, up to some level, you can three more. So to try out practically mining techniques to follow this practical. So that is again, step by step guide. And I have uploaded some datasets in addition to that, actually, there are some other techniques there to discretize data. So if I tell you one technique is Chris, mainly Let's say we can build hierarchies, let me take concept hierarchy so that mainly we will discuss with data engineer inside you can have hierarchy. Let's say we take date or we want to have month and details. So we can have a hierarchy like year inside the year. We have month inside the month, we have week inside the we have day and then time. Like, we can build hierarchy. So when we take cheaper. So maybe we can build another high graph is like, and these are in a different way of group in data. So after group, you can reduce the number of data points. Also , there are a lot of techniques available. So one example, the country country has province or cities cities has been treated. So we can we can in this example, you can see country if you look at data country point of view, maybe 15 Bisti values, but province levels. So more data city level, more data with level, more data. For some analysis, maybe we want to process data in different ways. So the maybe consider Iraq, sometimes. Maybe a solution. All right. So this is the data reduction and data integration is in another area. So I don't want to move into those things, but I kept Leslie here because it's good if you can just read it once just to get some idea. No need to go into details for this module, but just read that sometimes. Maybe those things are useful for you. Right. So for this course, actually, you want to mainly the first two points. Right. And do the pack pickers. Also. Sorry. First two points, not data reduction. Data data transformation. That is important. Right. So I want to give you time to do the practicals, but that one thing you want to do in addition to that. So we want to in this module. Actually, math goes for the assignment. Right. And for the final paper, since we have done some algorithms, now we can do some assignments . I have prepared one assignment from Association rule mining. I guess it's not that difficult. So I have given me open that assignment first. The assignment you can see deadline I set end of this month, actually. So then you get three weeks, I guess. Yes. You get exactly three weeks complete this assignment. I like to I like if you will do it as a group work. So maybe two or 3 members together can try out this. Then maybe you can share the knowledge with some others as well . But to me I want to know who are the people working together. So maybe your proof has two or 3 members. So let me know. And after that you can start working on so this assignment is rate from your final grade. And after complete this you want to submit a report and your solution and we can have you are submitting your report and you can demonstrate your maybe we can prepare you can prepare a small video clip and we can discuss that later. Right. So how to do the demonstration then what I want you to do, go to this web page that has some data set prepared for Association rule mining. So you can Select one of the data set. Right? So feel free to Select whatever you like to use and decide based on the data set, what kind of problem they may have. And based on that , think about use in Association rule mining. We can give a solution for them and you should be able to justify this problem we can solve using Association rule mining and you can analyze that selected data set and you can solve the resource. So you can use Vaca or Python or some other tools if you like to use free to use whatever the comfortable tool to you and do the demonstrations. But today what you want to do, you want to discuss it. We have one more hour form groups and go through the data set and after that have to understand the data set. So then you can think about some solution. You can develop that data set and after that develop statement of document. That means breakdown background. And I got off the work and different activities for doing the approach you are following and what? Prepare a small report and upload this  one. Right. So then I know what the members and all. So if I set time three PM. If that time given is not enough for you, I can extend time one number. Is it enough for this or you want to give more time? What is your idea now here today lecture. Only five of them are here. Right? I think seven or 8 previous lectures usually. So net there alright need more time. Okay. So if I said today midnight, is it okay? Just you don't have to give a solution today. Okay? Just prepare a report and tell me what you are doing and what is the data set you are using and what is your team and those details and what is your plan? So next hour you won't spend on this work. That's the target activity. So is it okay? Okay, you can unmute tell me your opinion because we have to do a few assignments. Otherwise you will get stuck in at the end. So this is so we have that more to cover. So then they want to be a big project or is toward signment . So that's what I'm thinking. Like we have one more month. So end of May, we want to finish this course. So therefore it's good to finish one assignment, end of April and you have then sometime in the next month to do the rest of the thing. Right. Okay. So I will extend deadline to submit this one. It's a statement of work today midnight. So if you couldn't complete it, just let me know. So then I can extend that further even. But it's been next hour or hour an hour, you get close to 50 minutes. You want to form groups and start working on this. How you're going to form groups or I guess all of you know each other. Right. Those are the practical problems when you do online more all of you know each other . This group has totally students from different pages and don't know each other. Is it mean I don't know each other? Is it. It the problem. So if so, then I will create I will create groups. Is it okay if I let you to form groups or do you want me to form groups? I don't know each other. So then. So then Let's do it this way. I will form. Okay. Just here on the file. So Let's form two groups, like three members in one and other one is I will create a breakout rooms randomly . So then you can two groups, 2 rooms. So then you can start. I don't know who is coming to, which then I will do a random way. So then after that, you can start work on this. Is that fine with everyone? I need your feedback. Alright. Okay. So is everyone agree? Okay. Okay. So. Okay. So then already there for me group. So her and near Ocean. So they like to work together. Okay. So then other three in the second group. Is that okay? Alright. So let me create groups. So lair and the member earning two of you wants to work together. And so then I can. So one member left. Is it what happens to live? So I will take time for the break crowd room 50 minutes on her left. Actually, I do know. Yeah. If you all know each other, you can create a different what is it? How? Okay, so I have two friends and they are not in the rich. I will upload. Okay. So some of them like to work with the non. So then I'll do in this meeting, I will upload a word document. So you want to type the group members. So in case if someone couldn't find members, so I will assign him to that group because only we have close to eight or 9 students. Two in that way. So in the course page, I will give me two or 3 minutes, I will upload a link. So you want to go to that link and type there a Brook Members. Okay, alright, so be online. So I will create that one drive 1. So you want to us sleep a Student ID to log in to that link, so then otherwise you will not get access. So let me create that link and get back to you soon.